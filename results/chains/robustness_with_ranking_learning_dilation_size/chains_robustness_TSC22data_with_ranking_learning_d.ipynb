{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain (directional trend) # \n",
    "Every pair of consecutive subsequences in a chain must be connected by both a forward arrow and a backward arrow. The key component of drifting is that the time series must contain chains with clear directionality\n",
    "\n",
    "Stumpy Tutorial Time Series Chains:\n",
    "https://stumpy.readthedocs.io/en/latest/Tutorial_Time_Series_Chains.html\n",
    "\n",
    "Matrix Profile VII: Time Series Chains Calibration Instruction:\n",
    "https://docs.google.com/presentation/d/1-jEynFIkjDR88QFtbHN2Iz8DXY8wMVet/edit#slide=id.p1\n",
    "\n",
    "Robust Time Series Chain Discovery with Incremental Nearest Neighbors:\n",
    "https://sites.google.com/view/robust-time-series-chain-22 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.testdata as testdata\n",
    "import core.utils as utils\n",
    "import core.calculate as calculate\n",
    "import core.visualize as visualize\n",
    "import core.results as results\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Robustness with Ranking and Learning the best dilation size with quality metrics ##\n",
    "\n",
    "recall and precision: hits are with overlap > 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "use_case = \"chains\"\n",
    "data_names = ['BME_1', 'BME_2', 'BME_3', 'BME_4', 'BME_5', 'CBF_1', 'CBF_2', 'CBF_3', 'CBF_4', 'CBF_5', 'ChlorineConcentration_1', 'ChlorineConcentration_2', 'ChlorineConcentration_3', 'ChlorineConcentration_4', 'ChlorineConcentration_5', 'ECG200_1', 'ECG200_2', 'ECG200_3', 'ECG200_4', 'ECG200_5', 'ECG5000_1', 'ECG5000_2', 'ECG5000_3', 'ECG5000_4', 'ECG5000_5', 'ECGFiveDays_1', 'ECGFiveDays_2', 'ECGFiveDays_3', 'ECGFiveDays_4', 'ECGFiveDays_5', 'FreezerRegularTrain_1', 'FreezerRegularTrain_2', 'FreezerRegularTrain_3', 'FreezerRegularTrain_4', 'FreezerRegularTrain_5', 'FreezerSmallTrain_1', 'FreezerSmallTrain_2', 'FreezerSmallTrain_3', 'FreezerSmallTrain_4', 'FreezerSmallTrain_5', 'Lightning7_1', 'Lightning7_2', 'Lightning7_3', 'Lightning7_4', 'Lightning7_5', 'Plane_1', 'Plane_2', 'Plane_3', 'Plane_4', 'Plane_5', 'SonyAIBORobotSurface1_1', 'SonyAIBORobotSurface1_2', 'SonyAIBORobotSurface1_3', 'SonyAIBORobotSurface1_4', 'SonyAIBORobotSurface1_5', 'SonyAIBORobotSurface2_1', 'SonyAIBORobotSurface2_2', 'SonyAIBORobotSurface2_3', 'SonyAIBORobotSurface2_4', 'SonyAIBORobotSurface2_5', 'Trace_1', 'Trace_2', 'Trace_3', 'Trace_4', 'Trace_5', 'TwoLeadECG_1', 'TwoLeadECG_2', 'TwoLeadECG_3', 'TwoLeadECG_4', 'TwoLeadECG_5', 'TwoPatterns_1', 'TwoPatterns_2', 'TwoPatterns_3', 'TwoPatterns_4', 'TwoPatterns_5', 'UMD_1', 'UMD_2', 'UMD_3', 'UMD_4', 'UMD_5', 'Wafer_1', 'Wafer_2', 'Wafer_3', 'Wafer_4', 'Wafer_5']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment 1/85: BME_1\n",
      "Starting Experiment 2/85: BME_2\n",
      "Starting Experiment 3/85: BME_3\n",
      "Starting Experiment 4/85: BME_4\n",
      "Starting Experiment 5/85: BME_5\n",
      "Starting Experiment 6/85: CBF_1\n",
      "Starting Experiment 7/85: CBF_2\n",
      "Starting Experiment 8/85: CBF_3\n",
      "Starting Experiment 9/85: CBF_4\n",
      "Starting Experiment 10/85: CBF_5\n",
      "Starting Experiment 11/85: ChlorineConcentration_1\n",
      "Starting Experiment 12/85: ChlorineConcentration_2\n",
      "Starting Experiment 13/85: ChlorineConcentration_3\n",
      "Starting Experiment 14/85: ChlorineConcentration_4\n",
      "Starting Experiment 15/85: ChlorineConcentration_5\n",
      "Starting Experiment 16/85: ECG200_1\n",
      "Starting Experiment 17/85: ECG200_2\n",
      "Starting Experiment 18/85: ECG200_3\n",
      "Starting Experiment 19/85: ECG200_4\n",
      "Starting Experiment 20/85: ECG200_5\n",
      "Starting Experiment 21/85: ECG5000_1\n",
      "Starting Experiment 22/85: ECG5000_2\n",
      "Starting Experiment 23/85: ECG5000_3\n",
      "Starting Experiment 24/85: ECG5000_4\n",
      "Starting Experiment 25/85: ECG5000_5\n",
      "Starting Experiment 26/85: ECGFiveDays_1\n",
      "Starting Experiment 27/85: ECGFiveDays_2\n",
      "Starting Experiment 28/85: ECGFiveDays_3\n",
      "Starting Experiment 29/85: ECGFiveDays_4\n",
      "Starting Experiment 30/85: ECGFiveDays_5\n",
      "Starting Experiment 31/85: FreezerRegularTrain_1\n",
      "Starting Experiment 32/85: FreezerRegularTrain_2\n",
      "Starting Experiment 33/85: FreezerRegularTrain_3\n",
      "Starting Experiment 34/85: FreezerRegularTrain_4\n",
      "Starting Experiment 35/85: FreezerRegularTrain_5\n",
      "Starting Experiment 36/85: FreezerSmallTrain_1\n",
      "Starting Experiment 37/85: FreezerSmallTrain_2\n",
      "Starting Experiment 38/85: FreezerSmallTrain_3\n",
      "Starting Experiment 39/85: FreezerSmallTrain_4\n",
      "Starting Experiment 40/85: FreezerSmallTrain_5\n",
      "Starting Experiment 41/85: Lightning7_1\n",
      "Starting Experiment 42/85: Lightning7_2\n",
      "Starting Experiment 43/85: Lightning7_3\n",
      "Starting Experiment 44/85: Lightning7_4\n",
      "Starting Experiment 45/85: Lightning7_5\n",
      "Starting Experiment 46/85: Plane_1\n",
      "Starting Experiment 47/85: Plane_2\n",
      "Starting Experiment 48/85: Plane_3\n",
      "Starting Experiment 49/85: Plane_4\n",
      "Starting Experiment 50/85: Plane_5\n",
      "Starting Experiment 51/85: SonyAIBORobotSurface1_1\n",
      "Starting Experiment 52/85: SonyAIBORobotSurface1_2\n",
      "Starting Experiment 53/85: SonyAIBORobotSurface1_3\n",
      "Starting Experiment 54/85: SonyAIBORobotSurface1_4\n",
      "Starting Experiment 55/85: SonyAIBORobotSurface1_5\n",
      "Starting Experiment 56/85: SonyAIBORobotSurface2_1\n",
      "Starting Experiment 57/85: SonyAIBORobotSurface2_2\n",
      "Starting Experiment 58/85: SonyAIBORobotSurface2_3\n",
      "Starting Experiment 59/85: SonyAIBORobotSurface2_4\n",
      "Starting Experiment 60/85: SonyAIBORobotSurface2_5\n",
      "Starting Experiment 61/85: Trace_1\n",
      "Starting Experiment 62/85: Trace_2\n",
      "Starting Experiment 63/85: Trace_3\n",
      "Starting Experiment 64/85: Trace_4\n",
      "Starting Experiment 65/85: Trace_5\n",
      "Starting Experiment 66/85: TwoLeadECG_1\n",
      "Starting Experiment 67/85: TwoLeadECG_2\n",
      "Starting Experiment 68/85: TwoLeadECG_3\n",
      "Starting Experiment 69/85: TwoLeadECG_4\n",
      "Starting Experiment 70/85: TwoLeadECG_5\n",
      "Starting Experiment 71/85: TwoPatterns_1\n",
      "Starting Experiment 72/85: TwoPatterns_2\n",
      "Starting Experiment 73/85: TwoPatterns_3\n",
      "Starting Experiment 74/85: TwoPatterns_4\n",
      "Starting Experiment 75/85: TwoPatterns_5\n",
      "Starting Experiment 76/85: UMD_1\n",
      "Starting Experiment 77/85: UMD_2\n",
      "Starting Experiment 78/85: UMD_3\n",
      "Starting Experiment 79/85: UMD_4\n",
      "Starting Experiment 80/85: UMD_5\n",
      "Starting Experiment 81/85: Wafer_1\n",
      "Starting Experiment 82/85: Wafer_2\n",
      "Starting Experiment 83/85: Wafer_3\n",
      "Starting Experiment 84/85: Wafer_4\n",
      "Starting Experiment 85/85: Wafer_5\n"
     ]
    }
   ],
   "source": [
    "max_dilation = 15\n",
    "\n",
    "cols = ['dataname', \"without dilation\", \"with dilation\"]\n",
    "\n",
    "recall_table = pd.DataFrame(columns=cols)\n",
    "precision_table = pd.DataFrame(columns=cols)\n",
    "f1_table = pd.DataFrame(columns=cols)\n",
    "\n",
    "# Bulk Experiment\n",
    "for count, data_name in enumerate(data_names):\n",
    "    print(f'Starting Experiment {count+1}/{len(data_names)}: {data_name}')\n",
    "    T = testdata.load_from_mat(\"../data/\" + use_case + \"/robustness/ts/\" + data_name + \".mat\", \"ts\")\n",
    "    l = testdata.load_from_mat(\"../data/\" + use_case + \"/robustness/ts/\" + data_name + \".mat\", \"l\")\n",
    "\n",
    "    ground_truth = testdata.load_gt(\"../data/\" + use_case + \"/robustness/gt/\" + data_name + \".mat\", \"idx_tsc\")\n",
    "\n",
    "    # Hyperparameter\n",
    "    target_w = int(l)\n",
    "    m = None\n",
    "    non_overlapping = False # if True, overlapping chains are filtered\n",
    "    offset = False # if offset=True, the chains with dilation are calculated with a starting offset of the chain without dilation\n",
    "\n",
    "    # calculate\n",
    "    # calculate.chains(T, max_dilation, data_name, use_case, ground_truth, offset, non_overlapping, target_w, m)\n",
    "\n",
    "    # evaluate by picking the scores from the dilation size with the best chain (here by correlation length)\n",
    "    recalls, precisions, f1_scores, correlation_lengths = utils.get_metrics_for_experiment(max_dilation, data_name, use_case, offset, non_overlapping, target_w, m, ground_truth)\n",
    "    index_best_chain_using_dilation = max(range(len(correlation_lengths[1:])), key=correlation_lengths[1:].__getitem__)\n",
    "\n",
    "    recall_row = [data_name, recalls[0], recalls[index_best_chain_using_dilation]]\n",
    "    recall_table.loc[len(recall_table)] = recall_row\n",
    "\n",
    "    precision_row = [data_name, precisions[0], precisions[index_best_chain_using_dilation]]\n",
    "    precision_table.loc[len(precision_table)] = precision_row\n",
    "\n",
    "    f1_row = [data_name, f1_scores[0], f1_scores[index_best_chain_using_dilation]]\n",
    "    f1_table.loc[len(f1_table)] = f1_row\n",
    "\n",
    "\n",
    "    # visualize:\n",
    "    # print(f'Ground Truth Chain: {ground_truth}')\n",
    "    # visualize.chains(max_dilation, data_name, use_case, offset, non_overlapping, target_w, m, ground_truth, visualize_chains=False)\n",
    "results.save_stats(recall_table, \"../results/chains/robustness_with_ranking_recalls_raw.csv\")\n",
    "results.save_stats(precision_table, \"../results/chains/robustness_with_ranking_precisions_raw.csv\")\n",
    "results.save_stats(f1_table, \"../results/chains/robustness_with_ranking_f1_scores_raw.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate raw csv results ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41689/3274523153.py:16: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  recalls_df = recalls_df.append(recalls_df_raw[i:i+5].mean(), ignore_index=True)\n",
      "/tmp/ipykernel_41689/3274523153.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  recalls_df = recalls_df.append(recalls_df_raw[i:i+5].mean(), ignore_index=True)\n",
      "/tmp/ipykernel_41689/3274523153.py:17: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  precisions_df = precisions_df.append(precisions_df_raw[i:i+5].mean(), ignore_index=True)\n",
      "/tmp/ipykernel_41689/3274523153.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  precisions_df = precisions_df.append(precisions_df_raw[i:i+5].mean(), ignore_index=True)\n",
      "/tmp/ipykernel_41689/3274523153.py:18: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  f1_scores_df = f1_scores_df.append(f1_scores_df_raw[i:i+5].mean(), ignore_index=True)\n",
      "/tmp/ipykernel_41689/3274523153.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  f1_scores_df = f1_scores_df.append(f1_scores_df_raw[i:i+5].mean(), ignore_index=True)\n",
      "/tmp/ipykernel_41689/3274523153.py:25: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  averages = df.mean()\n",
      "/tmp/ipykernel_41689/3274523153.py:25: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  averages = df.mean()\n",
      "/tmp/ipykernel_41689/3274523153.py:25: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  averages = df.mean()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "starting_strings = [name.split('_')[0] for name in data_names]\n",
    "data_names_condensed = starting_strings[0::5]\n",
    "\n",
    "recalls_df_raw = pd.read_csv(\"../results/chains/robustness_with_ranking_recalls_raw.csv\")\n",
    "precisions_df_raw = pd.read_csv(\"../results/chains/robustness_with_ranking_precisions_raw.csv\")\n",
    "f1_scores_df_raw = pd.read_csv(\"../results/chains/robustness_with_ranking_f1_scores_raw.csv\")\n",
    "\n",
    "recalls_df = pd.DataFrame(columns=recalls_df_raw.columns[1:])\n",
    "precisions_df = pd.DataFrame(columns=precisions_df_raw.columns[1:])\n",
    "f1_scores_df = pd.DataFrame(columns=f1_scores_df_raw.columns[1:])\n",
    "\n",
    "# build average performance for each dataset\n",
    "for i in range(0, len(recalls_df_raw), 5):\n",
    "    recalls_df = recalls_df.append(recalls_df_raw[i:i+5].mean(), ignore_index=True)\n",
    "    precisions_df = precisions_df.append(precisions_df_raw[i:i+5].mean(), ignore_index=True)\n",
    "    f1_scores_df = f1_scores_df.append(f1_scores_df_raw[i:i+5].mean(), ignore_index=True)\n",
    "\n",
    "recalls_df.insert(0, 'data_names', data_names_condensed)\n",
    "precisions_df.insert(0, 'data_names', data_names_condensed)\n",
    "f1_scores_df.insert(0, 'data_names', data_names_condensed)\n",
    "\n",
    "def add_av_performance_row(df):\n",
    "    averages = df.mean()\n",
    "    average_row = [\"AVERAGE PERFORMANCE\"] + averages.values.tolist()\n",
    "    df.loc[len(df)] = average_row\n",
    "    return df\n",
    "\n",
    "def add_wins_ties_row(df):\n",
    "    ties = [0] * (len(df.columns) - 1)\n",
    "    wins = [0] * (len(df.columns) - 1)\n",
    "    for index, row in df.iterrows():\n",
    "        # get maximum value in row\n",
    "        max_value = max(row[1:])\n",
    "        max_indices = [index - 1 for index, value in enumerate(row) if value == max_value]\n",
    "\n",
    "        if len(max_indices) == 1:\n",
    "            wins[max_indices[0]] += 1\n",
    "        else:\n",
    "            for i in max_indices:\n",
    "                ties[i] += 1\n",
    "\n",
    "    df.loc[len(df)] = [\"WINS\"] + wins\n",
    "    df.loc[len(df)] = [\"TIES\"] + ties\n",
    "    return df\n",
    "\n",
    "recalls_df = add_av_performance_row(recalls_df)\n",
    "recalls_df = add_wins_ties_row(recalls_df)\n",
    "precisions_df = add_av_performance_row(precisions_df)\n",
    "precisions_df = add_wins_ties_row(precisions_df)\n",
    "f1_scores_df = add_av_performance_row(f1_scores_df)\n",
    "f1_scores_df = add_wins_ties_row(f1_scores_df)\n",
    "\n",
    "# save results\n",
    "results.save_stats(recalls_df, \"../results/chains/robustness_with_ranking_recalls.csv\")\n",
    "results.save_stats(precisions_df, \"../results/chains/robustness_with_ranking_precisions.csv\")\n",
    "results.save_stats(f1_scores_df, \"../results/chains/robustness_with_ranking_f1_scores.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c63f775a29077d88326ae6f063688ea24fd4744bb6810a004c04d9663ba131b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
